{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d566a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shuffle_io] generated 10000/10223 shuffles\n",
      "[shuffle_io] wrote 10223 shuffles → S:\\Libraries\\OneDrive\\Documents\\Code Projects Parent Folder\\Code Projects\\Farkle Mk II\\notebooks\\data\\shuffles.bin (159.1 MiB)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from farkle.shuffle_io import read_shuffles, write_shuffle_file  # your shuffle_io.py\n",
    "\n",
    "from farkle.simulation import (\n",
    "    generate_strategy_grid,  # grid helper :contentReference[oaicite:0]{index=0}\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) PARAMETERS\n",
    "# -----------------------------------------------------------------------------\n",
    "n_strats   = 8_160               # size of the default grid\n",
    "n_shuffles = 10_223              # total number of random permutations you want\n",
    "shard_size = 1_000               # rows per shard\n",
    "out_dir    = Path(\"data/shards\") # where to drop the shards\n",
    "seed       = 42                  # any fixed seed for reproducibility\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) GENERATE MEMMAP SHUFFLE FILE\n",
    "# -----------------------------------------------------------------------------\n",
    "# This will create a raw binary file shaped (n_shuffles, n_strats), uint16.\n",
    "shuffle_file = write_shuffle_file(\n",
    "    path       = \"data/shuffles.bin\",\n",
    "    n_strats   = n_strats,\n",
    "    n_shuffles = n_shuffles,\n",
    "    seed       = seed,\n",
    ")  # produces data/shuffles.bin via streaming into a memmap\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) BUILD THE STRATEGY GRID ONCE\n",
    "# -----------------------------------------------------------------------------\n",
    "strategies, meta = generate_strategy_grid()\n",
    "# `strategies` is a List[ThresholdStrategy], length == n_strats :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) SHARD WRITER\n",
    "# -----------------------------------------------------------------------------\n",
    "def write_one_shard(shard_idx: int) -> Path:\n",
    "    \"\"\"\n",
    "    Read rows [start : start+count] from the memmap, map indices -> strategy\n",
    "    objects, and dump a compressed shard pickle.\n",
    "    \"\"\"\n",
    "    start = shard_idx * shard_size\n",
    "    count = min(shard_size, n_shuffles - start)\n",
    "    # zero-copy slice into a small ndarray of shape (count, n_strats)\n",
    "    perm_mat = read_shuffles(\n",
    "        shuffle_file,\n",
    "        start,\n",
    "        count   = count,\n",
    "        as_array=True\n",
    "    )\n",
    "    # perm_mat[i, j] is an integer in [0, n_strats)\n",
    "    # map each row to the actual ThresholdStrategy instances\n",
    "    rows: list[list] = [\n",
    "        [strategies[idx] for idx in perm_mat[i]]\n",
    "        for i in range(count)\n",
    "    ]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.columns = meta[\"strategy_idx\"]  # label columns by original strategy index\n",
    "    \n",
    "    # ensure output directory exists\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    shard_path = out_dir / f\"shard_{shard_idx:02d}.pkl.gz\"\n",
    "    # write compressed pickle\n",
    "    with gzip.open(shard_path, \"wb\") as f:\n",
    "        pickle.dump(df, f)\n",
    "    return shard_path\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) DISPATCH IN PARALLEL\n",
    "# -----------------------------------------------------------------------------\n",
    "n_shards = (n_shuffles + shard_size - 1) // shard_size\n",
    "\n",
    "with mp.Pool() as pool:\n",
    "    paths = pool.map(write_one_shard, range(n_shards))\n",
    "\n",
    "print(\"Wrote shards:\", paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f8906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:45:16 INFO  2510037422.py:89 | n_games_per_player = 10223, total tasks = 16683936\n",
      "16:45:16 DEBUG 2510037422.py:322 | about to call main from __main__  vars={'__name__': '__main__', '__doc__': '\\nRun the full Farkle strategy tournament with multiprocessing,…\n",
      "16:45:16 DEBUG 2510037422.py:221 | reached main  vars={}\n",
      "16:45:16 DEBUG 2510037422.py:168 | reached load_checkpoint  vars={}\n",
      "16:45:16 DEBUG 2510037422.py:171 | load_checkpoint found no checkpoint file  vars={'vars_snippet': '{}'}\n",
      "16:45:16 DEBUG 2510037422.py:224 | after load_checkpoint in main  vars={'vars_snippet': '{}', 'already_done': 0, 'win_counter': Counter()}\n",
      "16:45:16 DEBUG 2510037422.py:234 | created log_queue in main  vars={'vars_snippet': \"{'vars_snippet': '{}', 'already_done': 0, 'win_counter': Counter()}\", 'already_don…\n",
      "16:45:16 DEBUG 2510037422.py:238 | started listener process in main  vars={'vars_snippet': '{\\'vars_snippet\\': \"{\\'vars_snippet\\': \\'{}\\', \\'already_done\\': 0, \\'win_counter\\…\n",
      "16:45:16 DEBUG 2510037422.py:242 | recorded start_time in main  vars={'vars_snippet': '{\\'vars_snippet\\': \\'{\\\\\\'vars_snippet\\\\\\': \"{\\\\\\'vars_snippet\\\\\\': \\\\\\'{}\\\\\\', \\\\…\n",
      "16:45:16 DEBUG 2510037422.py:245 | initialized done counter in main  vars={'vars_snippet': '{\\'vars_snippet\\': \\'{\\\\\\'vars_snippet\\\\\\': \\\\\\'{\\\\\\\\\\\\\\'vars_snippet\\\\\\\\\\\\\\': \"{\\…\n",
      "16:45:16 DEBUG 2510037422.py:254 | entered Pool context in main  vars={'vars_snippet': '{\\'vars_snippet\\': \\'{\\\\\\'vars_snippet\\\\\\': \\\\\\'{\\\\\\\\\\\\\\'vars_snippet\\\\\\\\\\\\\\': \\\\\\…\n",
      "16:45:16 INFO  2510037422.py:255 | Pool started with 16 workers.\n",
      "16:45:16 DEBUG 2510037422.py:257 | logged pool start info in main  vars={'vars_snippet': \"{'vars_snippet': '{\\\\'vars_snippet\\\\': \\\\'{\\\\\\\\\\\\'vars_snippet\\\\\\\\\\\\': \\\\\\\\\\\\'{\\\\\\…\n",
      "16:45:16 DEBUG 2510037422.py:260 | created stream iterator in main  vars={'vars_snippet': '{\\'vars_snippet\\': \"{\\'vars_snippet\\': \\'{\\\\\\\\\\'vars_snippet\\\\\\\\\\': \\\\\\\\\\'{\\\\\\\\\\\\\\…\n",
      "16:45:16 DEBUG 2510037422.py:125 | reached task_stream:  vars={'already_done': 0}\n",
      "16:45:16 DEBUG 2510037422.py:135 | assigned task_stream variables vars={'already_done': 0, 'vars_snippet': \"{'already_done': 0}\", 'num_strats': 8160, 'tables_per_rep': 163…\n",
      "16:45:16 DEBUG 2510037422.py:139 | entered loop in task_stream   vars={'already_done': 0, 'vars_snippet': '{\\'already_done\\': 0, \\'vars_snippet\\': \"{\\'already_done\\': 0}\"…\n"
     ]
    }
   ],
   "source": [
    "# run_tournament_v2_debug_fixed.py\n",
    "\"\"\"\n",
    "Run the full Farkle strategy tournament with multiprocessing,\n",
    "queue-based logging, and resumable checkpoints.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "from farkle.stats import games_for_power\n",
    "\n",
    "from farkle.simulation import _play_game, generate_strategy_grid\n",
    "\n",
    "\n",
    "def short_vars(d: dict, max_len: int = 100) -> str:\n",
    "    s = repr(d)\n",
    "    return s if len(s) <= max_len else s[:max_len] + \"…\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 0.--- Globals & tuning knobs\n",
    "CHUNKSIZE       = 100          # tasks sent to each worker at a time\n",
    "PROCESSES       = 16\n",
    "MAXTASKS        = 50              # fork a fresh worker after this many tasks\n",
    "REPORT_EVERY    = 100_000         # progress print interval\n",
    "CHECKPOINT_FILE = Path(\"win_counter.chk\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1.--- Set up root logger and handler (no debug() calls here yet)\n",
    "DEBUG_FIRST_N = 10000   # change once; used by the filter below\n",
    "\n",
    "class FirstNFilter(logging.Filter):\n",
    "    \"\"\"\n",
    "    Let every unique call-site (file, lineno) speak `n` times, then silence it.\n",
    "    Works no matter how tight the surrounding loop is.\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int = DEBUG_FIRST_N):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.seen: Counter[Tuple[str, int]] = Counter()\n",
    "\n",
    "    def filter(self, record: logging.LogRecord) -> bool:\n",
    "        key = (record.pathname, record.lineno)\n",
    "        self.seen[key] += 1\n",
    "        return self.seen[key] <= self.n\n",
    "\n",
    "# configure the root logger\n",
    "root = logging.getLogger()             # grab root so *all* libraries inherit it\n",
    "root.setLevel(logging.DEBUG)           # we really want to see DEBUG\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)  # print to stdout\n",
    "handler.setFormatter(logging.Formatter(\n",
    "        \"%(asctime)s %(levelname)-5s %(filename)s:%(lineno)d | %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\"))\n",
    "handler.addFilter(FirstNFilter(DEBUG_FIRST_N))\n",
    "\n",
    "root.handlers[:] = [handler]            # replace any default handlers\n",
    "\n",
    "# now that the handler is fully set up, we can safely grab “tournament” logger\n",
    "log = logging.getLogger(\"tournament\")\n",
    "# No log.debug(...) here at module import time.\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s|%(processName)s|%(levelname)s|%(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2.--- Strategy grid and powered sample size\n",
    "strategies, meta = generate_strategy_grid()\n",
    "meta[\"str_repr\"] = [str(s) for s in strategies]\n",
    "\n",
    "n_games_per_player = games_for_power(\n",
    "    n_strategies=len(strategies),\n",
    "    delta=0.03, alpha=0.025, power=0.90,\n",
    "    method=\"bh\", pairwise=True,\n",
    ")\n",
    "total_tasks = len(strategies) * n_games_per_player // 5  # == total games\n",
    "\n",
    "log.info(\"n_games_per_player = %s, total tasks = %s\", n_games_per_player, total_tasks)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5.--- Worker function\n",
    "def _one(task):\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"reached _one  vars=%s\", vars_snippet)\n",
    "    seed, idxs = task\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"unpacked task in _one  vars=%s\", vars_snippet)\n",
    "    table = [strategies[i] for i in idxs]\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"constructed table in _one  vars=%s\", vars_snippet)\n",
    "    row = _play_game(seed, table, 10_000)\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"completed _play_game in _one  vars=%s\", vars_snippet)\n",
    "    win = str(row[f\"{row['winner']}_strategy\"])\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"computed win in _one  vars=%s\", vars_snippet)\n",
    "    return win\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3.–6. Combined, memory-light task stream with resume support\n",
    "\n",
    "def task_stream(already_done: int = 0):\n",
    "    \"\"\"\n",
    "    Lazily yields (seed, idxs) for each game:\n",
    "      - rep in [0..n_games_per_player)\n",
    "      - permute strategies\n",
    "      - take groups of 5 as one table\n",
    "    Skips the first `already_done` tables in O(1), not by calling next() repeatedly.\n",
    "    \"\"\"\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"reached task_stream:  vars=%s\", vars_snippet)\n",
    "    num_strats = len(strategies)\n",
    "    tables_per_rep = num_strats // 5\n",
    "\n",
    "    # figure out where to start\n",
    "    start_rep, start_table = divmod(already_done, tables_per_rep)\n",
    "\n",
    "    perm_rng = np.random.default_rng(999)       # RNG for the permutations\n",
    "    seed_rng = np.random.default_rng(1234)      # RNG for the per‐game seeds\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"assigned task_stream variables vars=%s\", vars_snippet)\n",
    "\n",
    "    for rep in range(start_rep, n_games_per_player):\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"entered loop in task_stream   vars=%s\", vars_snippet)\n",
    "        perm = perm_rng.permutation(num_strats)   # one new random order\n",
    "        # if this is our first rep, skip up to start_table\n",
    "        first_j = 5 * (start_table if rep == start_rep else 0)\n",
    "\n",
    "        for j in range(first_j, num_strats, 5):\n",
    "            # stop if not a full group of 5\n",
    "            if j + 5 > num_strats:\n",
    "                break\n",
    "            idxs = tuple(int(x) for x in perm[j : j + 5])\n",
    "            seed = int(seed_rng.integers(2**32))\n",
    "            yield (seed, idxs)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7.--- Checkpoint helpers\n",
    "def save_checkpoint(counter: Counter, done: int):\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"reached save_checkpoint  vars=%s\", vars_snippet)\n",
    "    with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "        pickle.dump({\"done\": done, \"counter\": dict(counter)}, f)\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"pickle.dump completed in save_checkpoint  vars=%s\", vars_snippet)\n",
    "    log.info(\"Checkpoint saved at %s tasks.\", done)\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"exiting save_checkpoint  vars=%s\", vars_snippet)\n",
    "\n",
    "def load_checkpoint() -> tuple[int, Counter]:\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"reached load_checkpoint  vars=%s\", vars_snippet)\n",
    "    if not CHECKPOINT_FILE.exists():\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"load_checkpoint found no checkpoint file  vars=%s\", vars_snippet)\n",
    "        return 0, Counter()\n",
    "    with CHECKPOINT_FILE.open(\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"pickle.load completed in load_checkpoint  vars=%s\", vars_snippet)\n",
    "    log.info(\"Checkpoint loaded: %s tasks finished previously.\", data[\"done\"])\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"exiting load_checkpoint with data  vars=%s\", vars_snippet)\n",
    "    return data[\"done\"], Counter(data[\"counter\"])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 8.--- Multiprocessing-friendly logging (single queue sink)\n",
    "def _configure_worker_logging(queue):\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"reached _configure_worker_logging  vars=%s\", vars_snippet)\n",
    "    qh = logging.handlers.QueueHandler(queue)\n",
    "    root = logging.getLogger()\n",
    "    root.handlers.clear()\n",
    "    root.setLevel(logging.DEBUG)\n",
    "    root.addHandler(qh)\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"configured worker logging handlers  vars=%s\", vars_snippet)\n",
    "\n",
    "def _listener_process(queue):\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"reached _listener_process  vars=%s\", vars_snippet)\n",
    "    h = logging.StreamHandler(sys.stdout)\n",
    "    fmt = logging.Formatter(\"%(asctime)s|%(processName)s|%(levelname)s|%(message)s\", \"%H:%M:%S\")\n",
    "    h.setFormatter(fmt)\n",
    "    root = logging.getLogger()\n",
    "    root.addHandler(h)\n",
    "    root.setLevel(logging.DEBUG)\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"listener configured its handler  vars=%s\", vars_snippet)\n",
    "    while True:\n",
    "        record = queue.get()\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"listener received record  vars=%s\", vars_snippet)\n",
    "        if record is None:\n",
    "            vars_snippet = short_vars(locals(), 100)                 \n",
    "            log.debug(\"listener received sentinel None  vars=%s\", vars_snippet)\n",
    "            break\n",
    "        root.handle(record)\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"listener exiting  vars=%s\", vars_snippet)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"reached main  vars=%s\", vars_snippet)\n",
    "    already_done, win_counter = load_checkpoint()\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"after load_checkpoint in main  vars=%s\", vars_snippet)\n",
    "\n",
    "    if already_done >= total_tasks:\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"main early exit condition met  vars=%s\", vars_snippet)\n",
    "        log.warning(\"All %s tasks are already complete. Nothing to do!\", total_tasks)\n",
    "        return\n",
    "\n",
    "    log_queue = mp.Queue()\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"created log_queue in main  vars=%s\", vars_snippet)\n",
    "    listener = mp.Process(target=_listener_process, args=(log_queue,), daemon=True)\n",
    "    listener.start()\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"started listener process in main  vars=%s\", vars_snippet)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"recorded start_time in main  vars=%s\", vars_snippet)\n",
    "    done = already_done\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"initialized done counter in main  vars=%s\", vars_snippet)\n",
    "\n",
    "    with mp.Pool(\n",
    "        processes=PROCESSES,\n",
    "        maxtasksperchild=MAXTASKS,\n",
    "        initializer=_configure_worker_logging,\n",
    "        initargs=(log_queue,),\n",
    "    ) as pool:\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"entered Pool context in main  vars=%s\", vars_snippet)\n",
    "        log.info(\"Pool started with %d workers.\", PROCESSES)\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"logged pool start info in main  vars=%s\", vars_snippet)\n",
    "        stream = task_stream(already_done)\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"created stream iterator in main  vars=%s\", vars_snippet)\n",
    "\n",
    "        for win in pool.imap_unordered(_one, stream, chunksize=CHUNKSIZE):\n",
    "            vars_snippet = short_vars(locals(), 100)                 \n",
    "            log.debug(\"received win from pool.imap_unordered  vars=%s\", vars_snippet)\n",
    "            win_counter[win] += 1\n",
    "            vars_snippet = short_vars(locals(), 100)                 \n",
    "            log.debug(\"updated win_counter in main loop  vars=%s\", vars_snippet)\n",
    "            done += 1\n",
    "            vars_snippet = short_vars(locals(), 100)                 \n",
    "            log.debug(\"incremented done in main loop  vars=%s\", vars_snippet)\n",
    "\n",
    "            if done % REPORT_EVERY == 0 or done == total_tasks:\n",
    "                vars_snippet = short_vars(locals(), 100)                 \n",
    "                log.debug(\"about to compute progress in main loop  vars=%s\", vars_snippet)\n",
    "                pct = 100 * done / total_tasks\n",
    "                hrs = (time.perf_counter() - start_time) / 3600\n",
    "                log.info(\"[%10d / %10d]  %6.2f %%  %6.2f h elapsed\", done, total_tasks, pct, hrs)\n",
    "                vars_snippet = short_vars(locals(), 100)                 \n",
    "                log.debug(\"about to save checkpoint in main loop  vars=%s\", vars_snippet)\n",
    "                save_checkpoint(win_counter, done)\n",
    "                vars_snippet = short_vars(locals(), 100)                 \n",
    "                log.debug(\"saved checkpoint in main loop  vars=%s\", vars_snippet)\n",
    "\n",
    "        vars_snippet = short_vars(locals(), 100)                 \n",
    "        log.debug(\"exited for loop in main  vars=%s\", vars_snippet)\n",
    "\n",
    "    # tell listener to finish\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"sending sentinel to listener  vars=%s\", vars_snippet)\n",
    "    log_queue.put(None)\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"sent sentinel to listener  vars=%s\", vars_snippet)\n",
    "    listener.join()\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"listener joined in main  vars=%s\", vars_snippet)\n",
    "\n",
    "    # final dump + CSV\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"about to save final checkpoint  vars=%s\", vars_snippet)\n",
    "    save_checkpoint(win_counter, done)\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"saved final checkpoint  vars=%s\", vars_snippet)\n",
    "\n",
    "    summary = (\n",
    "        meta[[\"strategy_idx\", \"str_repr\"]]\n",
    "        .assign(wincount=lambda df: df[\"str_repr\"].map(win_counter).fillna(0).astype(\"int32\"))\n",
    "        .sort_values(\"strategy_idx\")[[\"strategy_idx\", \"wincount\"]]\n",
    "    )\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"constructed summary DataFrame  vars=%s\", vars_snippet)\n",
    "    summary.to_csv(\"wincounts.csv\", index=False)\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"wrote wincounts.csv  vars=%s\", vars_snippet)\n",
    "    log.info(\"Finished!  CSV written with final results.\")\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"exiting main  vars=%s\", vars_snippet)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # At this point, the handler + root logger are fully configured.\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"about to call main from __main__  vars=%s\", vars_snippet)\n",
    "    main()\n",
    "    vars_snippet = short_vars(locals(), 100)                 \n",
    "    log.debug(\"returned from main in __main__  vars=%s\", vars_snippet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd7c380",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'ellipsis' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    113\u001b[39m strategies, meta = generate_strategy_grid()\n\u001b[32m    114\u001b[39m num_strats      = \u001b[38;5;28mlen\u001b[39m(strategies)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m n_games         = \u001b[43mgames_for_power\u001b[49m\u001b[43m(\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m total_tasks     = num_strats * n_games // \u001b[32m5\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Queues\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mS:\\Libraries\\OneDrive\\Documents\\Code Projects Parent Folder\\Code Projects\\Farkle Mk II\\src\\farkle\\stats.py:23\u001b[39m, in \u001b[36mgames_for_power\u001b[39m\u001b[34m(n_strategies, delta, base_p, alpha, power, method, pairwise)\u001b[39m\n\u001b[32m     21\u001b[39m     alpha_star = alpha / k\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mbh\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     h_m = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m/i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[43mn_strategies\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m))  \u001b[38;5;66;03m# harmonic number\u001b[39;00m\n\u001b[32m     24\u001b[39m     alpha_star = alpha / h_m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'ellipsis' and 'int'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import threading\n",
    "from pathlib import Path\n",
    "\n",
    "from farkle.stats import games_for_power\n",
    "\n",
    "from farkle.simulation import generate_strategy_grid\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Globals\n",
    "CHUNKSIZE       = 6\n",
    "QUEUE_MAXSIZE   = 100\n",
    "PROCESSES       = 16\n",
    "REPORT_every    = 100_000\n",
    "CHECKPOINT_FILE = Path(\"win_counter.chk\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def producer(task_q, n_games_per_player, num_strats):\n",
    "    \"\"\"Thread #1: generate (seed, idxs) and put into task_q, up to QUEUE_MAXSIZE.\"\"\"\n",
    "    perm_rng = np.random.default_rng(999)\n",
    "    seed_rng = np.random.default_rng(1234)\n",
    "\n",
    "    for _rep in range(n_games_per_player):\n",
    "        perm = perm_rng.permutation(num_strats)\n",
    "        # slice into 5‐player tables\n",
    "        for j in range(0, num_strats, 5):\n",
    "            if j + 5 > num_strats:\n",
    "                break\n",
    "            seed = int(seed_rng.integers(2**32))\n",
    "            task_q.put((seed, tuple(int(x) for x in perm[j:j+5])))\n",
    "    # signal end to workers\n",
    "    for _ in range(PROCESSES):\n",
    "        task_q.put(None)\n",
    "\n",
    "\n",
    "def worker(task_q, result_q):\n",
    "    \"\"\"N processes: pull tasks 6 at a time, run games, push wins.\"\"\"\n",
    "    from farkle.simulation import _play_game  # worker imports\n",
    "    while True:\n",
    "        batch = []\n",
    "        # gather up to CHUNKSIZE tasks\n",
    "        for _ in range(CHUNKSIZE):\n",
    "            task = task_q.get()\n",
    "            if task is None:\n",
    "                # re‐signal None so other workers see it\n",
    "                task_q.put(None)\n",
    "                break\n",
    "            batch.append(task)\n",
    "\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        # run each game in this batch\n",
    "        for seed, idxs in batch:\n",
    "            # reconstruct strategies list and play\n",
    "            row = _play_game(seed, [strategies[i] for i in idxs], 10_000)\n",
    "            win = str(row[f\"{row['winner']}_strategy\"])\n",
    "            result_q.put(win)\n",
    "    # when finished, let collector know\n",
    "    result_q.put(None)\n",
    "\n",
    "\n",
    "def collector(result_q, total_tasks):\n",
    "    \"\"\"Thread #2: pull wins, update counter, save checkpoints & CSV.\"\"\"\n",
    "    win_counter = Counter()\n",
    "    done = 0\n",
    "    active_workers = PROCESSES\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    while active_workers:\n",
    "        msg = result_q.get()\n",
    "        if msg is None:\n",
    "            active_workers -= 1\n",
    "            continue\n",
    "        win_counter[msg] += 1\n",
    "        done += 1\n",
    "\n",
    "        if done % REPORT_every == 0 or done == total_tasks:\n",
    "            hrs = (time.perf_counter() - start)/3600\n",
    "            print(f\"[{done}/{total_tasks}] {hrs:.2f} h elapsed\")\n",
    "            with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "                pickle.dump({\"done\": done, \"counter\": dict(win_counter)}, f)\n",
    "\n",
    "    # all workers done\n",
    "    # final CSV\n",
    "    import pandas as pd\n",
    "    summary = (\n",
    "      pd.DataFrame({\"strategy_idx\": range(num_strats),\n",
    "                    \"str_repr\": [str(s) for s in strategies]})\n",
    "        .assign(wincount=lambda df: df[\"str_repr\"].map(win_counter).fillna(0).astype(int))\n",
    "        .sort_values(\"strategy_idx\")\n",
    "    )\n",
    "    summary.to_csv(\"wincounts.csv\", index=False)\n",
    "    print(\"Finished! CSV written.\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    strategies, meta = generate_strategy_grid()\n",
    "    num_strats      = len(strategies)\n",
    "    n_games         = games_for_power(...)\n",
    "    total_tasks     = num_strats * n_games // 5\n",
    "\n",
    "    # Queues\n",
    "    task_q   = mp.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "    result_q = mp.Queue()\n",
    "\n",
    "    # Start producer thread\n",
    "    prod_thread = threading.Thread(target=producer,\n",
    "                                   args=(task_q, n_games, num_strats),\n",
    "                                   daemon=True)\n",
    "    prod_thread.start()\n",
    "\n",
    "    # Start collector thread\n",
    "    coll_thread = threading.Thread(target=collector,\n",
    "                                   args=(result_q, total_tasks),\n",
    "                                   daemon=True)\n",
    "    coll_thread.start()\n",
    "\n",
    "    # Spawn worker processes\n",
    "    processes = []\n",
    "    for _ in range(PROCESSES):\n",
    "        p = mp.Process(target=worker, args=(task_q, result_q))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    # Wait for everyone to finish\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    coll_thread.join()\n",
    "    print(\"All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cea21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import threading\n",
    "from pathlib import Path\n",
    "\n",
    "from farkle.stats import games_for_power\n",
    "\n",
    "from farkle.simulation import generate_strategy_grid\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Globals\n",
    "CHUNKSIZE       = 6\n",
    "QUEUE_MAXSIZE   = 100\n",
    "PROCESSES       = 16\n",
    "REPORT_EVERY    = 100_000\n",
    "CHECKPOINT_FILE = Path(\"win_counter.chk\")\n",
    "\n",
    "\n",
    "def producer(task_q: mp.Queue, n_games_per_player: int, num_strats: int) -> None:\n",
    "    \"\"\"Thread #1: generate (seed, idxs) and put into task_q, up to QUEUE_MAXSIZE.\"\"\"\n",
    "    perm_rng = np.random.default_rng(999)\n",
    "    seed_rng = np.random.default_rng(1234)\n",
    "\n",
    "    for _rep in range(n_games_per_player):\n",
    "        perm = perm_rng.permutation(num_strats)\n",
    "        for j in range(0, num_strats, 5):\n",
    "            if j + 5 > num_strats:\n",
    "                break\n",
    "            seed = int(seed_rng.integers(2**32))\n",
    "            task_q.put((seed, tuple(int(x) for x in perm[j : j + 5])))\n",
    "    # signal end to workers\n",
    "    for _ in range(PROCESSES):\n",
    "        task_q.put(None)\n",
    "\n",
    "\n",
    "def worker(task_q: mp.Queue, result_q: mp.Queue) -> None:\n",
    "    \"\"\"Process: pull tasks CHUNKSIZE at a time, run games, push wins.\"\"\"\n",
    "    from farkle.simulation import _play_game  # worker import\n",
    "\n",
    "    while True:\n",
    "        batch = []\n",
    "        for _ in range(CHUNKSIZE):\n",
    "            task = task_q.get()\n",
    "            if task is None:\n",
    "                task_q.put(None)\n",
    "                break\n",
    "            batch.append(task)\n",
    "\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        for seed, idxs in batch:\n",
    "            row = _play_game(seed, [strategies[i] for i in idxs], 10_000)\n",
    "            win = str(row[f\"{row['winner']}_strategy\"])\n",
    "            result_q.put(win)\n",
    "    # signal collector that this worker is done\n",
    "    result_q.put(None)\n",
    "\n",
    "\n",
    "def collector(result_q: mp.Queue, total_tasks: int) -> None:\n",
    "    \"\"\"Thread #2: pull wins, update counter, save checkpoints & CSV.\"\"\"\n",
    "    win_counter = Counter()\n",
    "    done = 0\n",
    "    active_workers = PROCESSES\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    while active_workers:\n",
    "        msg = result_q.get()\n",
    "        if msg is None:\n",
    "            active_workers -= 1\n",
    "            continue\n",
    "        win_counter[msg] += 1\n",
    "        done += 1\n",
    "\n",
    "        if done % REPORT_EVERY == 0 or done == total_tasks:\n",
    "            hrs = (time.perf_counter() - start) / 3600\n",
    "            print(f\"[{done}/{total_tasks}] {hrs:.2f} h elapsed\")\n",
    "            with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "                pickle.dump({\"done\": done, \"counter\": dict(win_counter)}, f)\n",
    "\n",
    "    # all workers done → final CSV\n",
    "    summary = (\n",
    "        pd.DataFrame({\n",
    "            \"strategy_idx\": range(num_strats),\n",
    "            \"str_repr\": [str(s) for s in strategies]\n",
    "        })\n",
    "        .assign(wincount=lambda df: df[\"str_repr\"].map(win_counter).fillna(0).astype(int))\n",
    "        .sort_values(\"strategy_idx\")\n",
    "    )\n",
    "    summary.to_csv(\"wincounts.csv\", index=False)\n",
    "    print(\"Finished! CSV written.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare strategy grid and determine total tasks\n",
    "    strategies, meta = generate_strategy_grid()\n",
    "    num_strats = len(strategies)\n",
    "    n_games_per_player = games_for_power(\n",
    "        n_strategies=num_strats,\n",
    "        delta=0.03, alpha=0.025, power=0.90,\n",
    "        method=\"bh\", pairwise=True,\n",
    "    )\n",
    "    total_tasks = num_strats * n_games_per_player // 5\n",
    "\n",
    "    # Queues\n",
    "    task_q = mp.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "    result_q = mp.Queue()\n",
    "\n",
    "    # Start producer thread\n",
    "    prod_thread = threading.Thread(\n",
    "        target=producer,\n",
    "        args=(task_q, n_games_per_player, num_strats),\n",
    "        daemon=True\n",
    "    )\n",
    "    prod_thread.start()\n",
    "\n",
    "    # Start collector thread\n",
    "    coll_thread = threading.Thread(\n",
    "        target=collector,\n",
    "        args=(result_q, total_tasks),\n",
    "        daemon=True\n",
    "    )\n",
    "    coll_thread.start()\n",
    "\n",
    "    # Spawn worker processes\n",
    "    processes = []\n",
    "    for _ in range(PROCESSES):\n",
    "        p = mp.Process(target=worker, args=(task_q, result_q))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    # Wait for workers and collector to finish\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    coll_thread.join()\n",
    "    print(\"All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:52:38 DEBUG 1541090730.py:184 | reached __main__ start vars={'__name__': '__main__', '__doc__': '\\nRun the full Farkle strategy tournament with threading, multi…\n",
      "17:52:38 DEBUG 1541090730.py:194 | computed grid vars={'__name__': '__main__', '__doc__': '\\nRun the full Farkle strategy tournament with threading, multi…\n",
      "17:52:38 DEBUG 1541090730.py:198 | queues created vars={'__name__': '__main__', '__doc__': '\\nRun the full Farkle strategy tournament with threading, multi…\n",
      "17:52:38 DEBUG 1541090730.py:76 | reached producer start vars={'task_q': <multiprocessing.queues.Queue object at 0x000001F77877E390>, 'n_games_per_player': 10223,…\n",
      "17:52:38 DEBUG 1541090730.py:206 | producer thread started\n",
      "17:52:38 DEBUG 1541090730.py:80 | initialized producer RNGs vars={'task_q': <multiprocessing.queues.Queue object at 0x000001F77877E390>, 'n_games_per_player': 10223,…\n",
      "17:52:38 DEBUG 1541090730.py:84 | producer loop rep=0 vars={'task_q': <multiprocessing.queues.Queue object at 0x000001F77877E390>, 'n_games_per_player': 10223,…\n",
      "17:52:38 DEBUG 1541090730.py:141 | reached collector start vars={'result_q': <multiprocessing.queues.Queue object at 0x000001F777097320>, 'total_tasks': 16683936}\n",
      "17:52:38 DEBUG 1541090730.py:214 | collector thread started\n",
      "17:52:39 DEBUG 1541090730.py:221 | worker processes started count=16\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Run the full Farkle strategy tournament with threading, multiprocessing,\n",
    "producer–worker–collector pattern, queue-based logging, and resumable checkpoints.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import threading\n",
    "from pathlib import Path\n",
    "\n",
    "from farkle.stats import games_for_power\n",
    "\n",
    "from farkle.simulation import generate_strategy_grid\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def short_vars(d: dict, max_len: int = 100) -> str:\n",
    "    s = repr(d)\n",
    "    return s if len(s) <= max_len else s[:max_len] + \"…\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 0.--- Globals & tuning knobs\n",
    "CHUNKSIZE       = 6           # tasks sent to each worker at a time\n",
    "QUEUE_MAXSIZE   = 100         # max tasks buffered\n",
    "PROCESSES       = 16\n",
    "REPORT_EVERY    = 100_000     # progress print interval\n",
    "CHECKPOINT_FILE = Path(\"win_counter.chk\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1.--- Set up root logger and handler\n",
    "DEBUG_FIRST_N = 10000   # change once; used by the filter below\n",
    "\n",
    "class FirstNFilter(logging.Filter):\n",
    "    \"\"\"\n",
    "    Let every unique call-site (file, lineno) speak `n` times, then silence it.\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int = DEBUG_FIRST_N):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.seen: Counter[Tuple[str, int]] = Counter()\n",
    "\n",
    "    def filter(self, record: logging.LogRecord) -> bool:\n",
    "        key = (record.pathname, record.lineno)\n",
    "        self.seen[key] += 1\n",
    "        return self.seen[key] <= self.n\n",
    "\n",
    "# configure the root logger\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(\n",
    "    \"%(asctime)s %(levelname)-5s %(filename)s:%(lineno)d | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    "))\n",
    "handler.addFilter(FirstNFilter(DEBUG_FIRST_N))\n",
    "root.handlers[:] = [handler]\n",
    "\n",
    "# tournament logger\n",
    "log = logging.getLogger(\"tournament\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2.--- Producer: enqueue tasks\n",
    "\n",
    "def producer(task_q: mp.Queue, n_games_per_player: int, num_strats: int) -> None:\n",
    "    vars_snippet = short_vars(locals(), 100)\n",
    "    log.debug(\"reached producer start vars=%s\", vars_snippet)\n",
    "    perm_rng = np.random.default_rng(999)\n",
    "    seed_rng = np.random.default_rng(1234)\n",
    "    vars_snippet = short_vars(locals(), 100)\n",
    "    log.debug(\"initialized producer RNGs vars=%s\", vars_snippet)\n",
    "\n",
    "    for rep in range(n_games_per_player):\n",
    "        vars_snippet = short_vars(locals(), 100)\n",
    "        log.debug(\"producer loop rep=%d vars=%s\", rep, vars_snippet)\n",
    "        perm = perm_rng.permutation(num_strats)\n",
    "        for j in range(0, num_strats, 5):\n",
    "            vars_snippet = short_vars(locals(), 100)\n",
    "            # log.debug(\"producer inner loop j=%d vars=%s\", j, vars_snippet)\n",
    "            if j + 5 > num_strats:\n",
    "                log.debug(\"producer breaking inner loop at j=%d\", j)\n",
    "                break\n",
    "            seed = int(seed_rng.integers(2**32))\n",
    "            task = (seed, tuple(int(x) for x in perm[j : j + 5]))\n",
    "            task_q.put(task)\n",
    "    vars_snippet = short_vars(locals(), 100)\n",
    "    log.debug(\"producer signaling end vars=%s\", vars_snippet)\n",
    "    for _ in range(PROCESSES):\n",
    "        task_q.put(None)\n",
    "    log.debug(\"producer done\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3.--- Worker processes: consume tasks\n",
    "\n",
    "def worker(task_q: mp.Queue, result_q: mp.Queue) -> None:\n",
    "    vars_snippet = short_vars(locals(), 100)\n",
    "    log.debug(\"reached worker start vars=%s\", vars_snippet)\n",
    "    from farkle.simulation import _play_game  # delay import in worker\n",
    "\n",
    "    while True:\n",
    "        batch: list[tuple[int, tuple[int, ...]]] = []\n",
    "        for _ in range(CHUNKSIZE):\n",
    "            task = task_q.get()\n",
    "            vars_snippet = short_vars(locals(), 100)\n",
    "            log.debug(\"worker got task vars=%s\", vars_snippet)\n",
    "            if task is None:\n",
    "                log.debug(\"worker received None sentinel\")\n",
    "                task_q.put(None)\n",
    "                break\n",
    "            batch.append(task)\n",
    "        vars_snippet = short_vars(locals(), 100)\n",
    "        log.debug(\"worker batch size=%d vars=%s\", len(batch), vars_snippet)\n",
    "        if not batch:\n",
    "            log.debug(\"worker breaking - empty batch\")\n",
    "            break\n",
    "\n",
    "        for seed, idxs in batch:\n",
    "            vars_snippet = short_vars(locals(), 100)\n",
    "            log.debug(\"worker playing game seed=%d idxs=%s vars=%s\", seed, idxs, vars_snippet)\n",
    "            row = _play_game(seed, [strategies[i] for i in idxs], 10_000)\n",
    "            win = str(row[f\"{row['winner']}_strategy\"])\n",
    "            result_q.put(win)\n",
    "    vars_snippet = short_vars(locals(), 100)\n",
    "    log.debug(\"worker sending None to collector vars=%s\", vars_snippet)\n",
    "    result_q.put(None)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4.--- Collector: tally wins, checkpoints, and CSV\n",
    "\n",
    "def collector(result_q: mp.Queue, total_tasks: int) -> None:\n",
    "    vars_snippet = short_vars(locals(), 100)\n",
    "    log.debug(\"reached collector start vars=%s\", vars_snippet)\n",
    "    win_counter = Counter()\n",
    "    done = 0\n",
    "    active_workers = PROCESSES\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    while active_workers:\n",
    "        msg = result_q.get()\n",
    "        vars_snippet = short_vars(locals(), 100)\n",
    "        log.debug(\"collector got msg vars=%s\", vars_snippet)\n",
    "        if msg is None:\n",
    "            active_workers -= 1\n",
    "            log.debug(\"collector worker done, active_workers=%d\", active_workers)\n",
    "            continue\n",
    "        win_counter[msg] += 1\n",
    "        done += 1\n",
    "\n",
    "        if done % REPORT_EVERY == 0 or done == total_tasks:\n",
    "            hrs = (time.perf_counter() - start) / 3600\n",
    "            log.info(\"[%d/%d] %6.2f h elapsed\", done, total_tasks, hrs)\n",
    "            with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "                pickle.dump({\"done\": done, \"counter\": dict(win_counter)}, f)\n",
    "            log.debug(\"collector saved checkpoint at done=%d\", done)\n",
    "\n",
    "    vars_snippet = short_vars(locals(), 100)\n",
    "    log.debug(\"collector constructing final summary vars=%s\", vars_snippet)\n",
    "    summary = (\n",
    "        pd.DataFrame({\n",
    "            \"strategy_idx\": range(num_strats),\n",
    "            \"str_repr\": [str(s) for s in strategies]\n",
    "        })\n",
    "        .assign(wincount=lambda df: df[\"str_repr\"].map(win_counter).fillna(0).astype(int))\n",
    "        .sort_values(\"strategy_idx\")\n",
    "    )\n",
    "    log.debug(\"collector writing final CSV\")\n",
    "    summary.to_csv(\"wincounts.csv\", index=False)\n",
    "    log.info(\"Finished! CSV written.\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5.--- Main entrypoint\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vars_snippet = short_vars(locals(), 100)\n",
    "    log.debug(\"reached __main__ start vars=%s\", vars_snippet)\n",
    "\n",
    "    strategies, meta = generate_strategy_grid()\n",
    "    num_strats = len(strategies)\n",
    "    n_games_per_player = games_for_power(\n",
    "        n_strategies=num_strats,\n",
    "        delta=0.03, alpha=0.025, power=0.90,\n",
    "        method=\"bh\", pairwise=True,\n",
    "    )\n",
    "    total_tasks = num_strats * n_games_per_player // 5\n",
    "    log.debug(\"computed grid vars=%s\", short_vars(locals(), 100))\n",
    "\n",
    "    task_q = mp.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "    result_q = mp.Queue()\n",
    "    log.debug(\"queues created vars=%s\", short_vars(locals(), 100))\n",
    "\n",
    "    prod_thread = threading.Thread(\n",
    "        target=producer,\n",
    "        args=(task_q, n_games_per_player, num_strats),\n",
    "        daemon=True\n",
    "    )\n",
    "    prod_thread.start()\n",
    "    log.debug(\"producer thread started\")\n",
    "\n",
    "    coll_thread = threading.Thread(\n",
    "        target=collector,\n",
    "        args=(result_q, total_tasks),\n",
    "        daemon=True\n",
    "    )\n",
    "    coll_thread.start()\n",
    "    log.debug(\"collector thread started\")\n",
    "\n",
    "    processes: list[mp.Process] = []\n",
    "    for _ in range(PROCESSES):\n",
    "        p = mp.Process(target=worker, args=(task_q, result_q))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    log.debug(\"worker processes started count=%d\", len(processes))\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    coll_thread.join()\n",
    "    log.debug(\"all processes and collector joined\")\n",
    "    log.info(\"All done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73fc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:13:21 DEBUG | 4021196756.py:134 | main start\n",
      "18:13:21 INFO  | 4021196756.py:146 | Grid: 8160 strategies, 10223 games/strat ⇒ 16683936 tasks.\n",
      "18:13:21 DEBUG | 4021196756.py:42 | producer start n_games_per_player=10223 num_strats=8160\n",
      "18:13:21 DEBUG | 4021196756.py:151 | producer thread started\n",
      "18:13:21 DEBUG | 4021196756.py:45 | producer RNGs initialized\n",
      "18:13:21 DEBUG | 4021196756.py:104 | collector start\n",
      "18:13:21 DEBUG | 4021196756.py:153 | collector thread started\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from farkle.stats import games_for_power\n",
    "\n",
    "from farkle.simulation import generate_strategy_grid\n",
    "\n",
    "CHUNKSIZE = 200\n",
    "FLUSH_EVERY = 1000\n",
    "PROCESSES = 16\n",
    "QUEUE_MAXSIZE = 50_000\n",
    "REPORT_EVERY = 100_000\n",
    "CHECKPOINT_FILE = Path(\"win_counter.chk\")\n",
    "DEBUG_FIRST_N = 10000\n",
    "\n",
    "class FirstNFilter(logging.Filter):\n",
    "    def __init__(self, n: int = DEBUG_FIRST_N):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.seen: Counter[Tuple[str, int]] = Counter()\n",
    "    def filter(self, record: logging.LogRecord) -> bool:\n",
    "        key = (record.pathname, record.lineno)\n",
    "        self.seen[key] += 1\n",
    "        return self.seen[key] <= self.n\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(\n",
    "    \"%(asctime)s %(levelname)-5s | %(filename)s:%(lineno)d | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    "))\n",
    "handler.addFilter(FirstNFilter())\n",
    "root.handlers[:] = [handler]\n",
    "log = logging.getLogger(\"tournament\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "def producer(task_q: mp.Queue, n_games_per_player: int, num_strats: int) -> None:\n",
    "    log.debug(\"producer start n_games_per_player=%d num_strats=%d\", n_games_per_player, num_strats)\n",
    "    perm_rng = np.random.default_rng(999)\n",
    "    seed_rng = np.random.default_rng(1234)\n",
    "    log.debug(\"producer RNGs initialized\")\n",
    "    for _rep in range(n_games_per_player):\n",
    "        # log.debug(\"producer iteration rep=%d\", rep)\n",
    "        perm = perm_rng.permutation(num_strats)\n",
    "        for j in range(0, num_strats, 5):\n",
    "            # log.debug(\"producer inner j=%d\", j)\n",
    "            if j + 5 > num_strats:\n",
    "                log.debug(\"producer breaking inner loop at j=%d\", j)\n",
    "                break\n",
    "            seed = int(seed_rng.integers(2**32))\n",
    "            task = (seed, tuple(int(x) for x in perm[j:j+5]))\n",
    "            task_q.put(task)\n",
    "            # log.debug(\"producer enqueued task %s\", task)\n",
    "    for _ in range(PROCESSES):\n",
    "        task_q.put(None)\n",
    "        log.debug(\"producer sent sentinel None\")\n",
    "    log.debug(\"producer done\")\n",
    "\n",
    "def worker(strat_list: List, task_q: mp.Queue, result_q: mp.Queue) -> None:\n",
    "    log.debug(\"worker start\")\n",
    "    global strategies\n",
    "    strategies = strat_list\n",
    "    local_counter: Counter[str] = Counter()\n",
    "    processed = 0\n",
    "    while True:\n",
    "        batch: List[Tuple[int, Tuple[int, ...]]] = []\n",
    "        sentinel = False\n",
    "        for _ in range(CHUNKSIZE):\n",
    "            task = task_q.get()\n",
    "            log.debug(\"worker got task %s\", task)\n",
    "            if task is None:\n",
    "                log.debug(\"worker received sentinel None\")\n",
    "                sentinel = True\n",
    "                break\n",
    "            batch.append(task)\n",
    "        log.debug(\"worker batch size=%d\", len(batch))\n",
    "        if not batch and sentinel:\n",
    "            log.debug(\"worker exiting: empty batch and sentinel\")\n",
    "            break\n",
    "        for seed, idxs in batch:\n",
    "            row = _play_game(seed, [strategies[i] for i in idxs], 10_000)\n",
    "            winner_key = str(row[f\"{row['winner']}_strategy\"])\n",
    "            local_counter[winner_key] += 1\n",
    "            processed += 1\n",
    "            log.debug(\"worker processed game %d winner=%s\", processed, winner_key)\n",
    "            if processed % FLUSH_EVERY == 0:\n",
    "                result_q.put(local_counter)\n",
    "                log.debug(\"worker flushed local_counter at processed=%d\", processed)\n",
    "                local_counter = Counter()\n",
    "        if sentinel:\n",
    "            log.debug(\"worker exiting after sentinel\")\n",
    "            break\n",
    "    if local_counter:\n",
    "        result_q.put(local_counter)\n",
    "        log.debug(\"worker final flush local_counter\")\n",
    "    result_q.put(None)\n",
    "    log.debug(\"worker sent final sentinel None\")\n",
    "\n",
    "def collector(result_q: mp.Queue) -> None:\n",
    "    log.debug(\"collector start\")\n",
    "    win_counter: Counter[str] = Counter()\n",
    "    done_batches = 0\n",
    "    active_workers = PROCESSES\n",
    "    start = time.perf_counter()\n",
    "    while active_workers:\n",
    "        msg = result_q.get()\n",
    "        log.debug(\"collector got msg %s\", msg)\n",
    "        if msg is None:\n",
    "            active_workers -= 1\n",
    "            log.debug(\"collector worker done, active_workers=%d\", active_workers)\n",
    "            continue\n",
    "        win_counter.update(msg)\n",
    "        done_batches += 1\n",
    "        log.debug(\"collector updated win_counter with batch, done_batches=%d\", done_batches)\n",
    "        if done_batches % REPORT_EVERY == 0 or active_workers == 0:\n",
    "            hrs = (time.perf_counter() - start) / 3600\n",
    "            log.info(\"[batch %d] %.2f h elapsed\", done_batches, hrs)\n",
    "            with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "                pickle.dump({\"done\": done_batches, \"counter\": dict(win_counter)}, f)\n",
    "            log.debug(\"collector saved checkpoint at done_batches=%d\", done_batches)\n",
    "    df = pd.DataFrame({\n",
    "        \"strategy_idx\": range(len(strategies)),\n",
    "        \"str_repr\": [str(s) for s in strategies],\n",
    "    })\n",
    "    df[\"wincount\"] = df[\"str_repr\"].map(win_counter).fillna(0).astype(int)\n",
    "    df.to_csv(\"wincounts.csv\", index=False)\n",
    "    log.info(\"collector CSV written\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    log.debug(\"main start\")\n",
    "    strategies, _ = generate_strategy_grid()\n",
    "    num_strats = len(strategies)\n",
    "    n_games_per_player = games_for_power(\n",
    "        n_strategies=num_strats,\n",
    "        delta=0.03,\n",
    "        alpha=0.025,\n",
    "        power=0.90,\n",
    "        method=\"bh\",\n",
    "        pairwise=True,\n",
    "    )\n",
    "    total_tasks = num_strats * n_games_per_player // 5\n",
    "    log.info(\"Grid: %d strategies, %d games/strat ⇒ %d tasks.\", num_strats, n_games_per_player, total_tasks)\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    task_q = ctx.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "    result_q = ctx.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "    threading.Thread(target=producer, args=(task_q, n_games_per_player, num_strats), daemon=True).start()\n",
    "    log.debug(\"producer thread started\")\n",
    "    threading.Thread(target=collector, args=(result_q,), daemon=True).start()\n",
    "    log.debug(\"collector thread started\")\n",
    "    processes = [ctx.Process(target=worker, args=(strategies, task_q, result_q)) for _ in range(PROCESSES)]\n",
    "    for p in processes:\n",
    "        p.start()\n",
    "        log.debug(\"worker process started pid=%s\", p.pid)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        log.debug(\"worker process joined pid=%s\", p.pid)\n",
    "    log.info(\"All workers joined – tournament complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:44:36 DEBUG | 1531619527.py:150 | main start\n",
      "18:44:36 INFO  | 1531619527.py:162 | Grid: 8160 strategies, 10223 games/strat ⇒ 16683936 tasks.\n",
      "18:44:36 DEBUG | 1531619527.py:57 | producer start n_games_per_player=10223 num_strats=8160\n",
      "18:44:36 DEBUG | 1531619527.py:167 | producer thread started\n",
      "18:44:36 DEBUG | 1531619527.py:60 | producer RNGs initialized\n",
      "18:44:36 DEBUG | 1531619527.py:120 | collector start\n",
      "18:44:36 DEBUG | 1531619527.py:169 | collector thread started\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import threading\n",
    "from pathlib import Path\n",
    "\n",
    "from farkle.stats import games_for_power\n",
    "\n",
    "from farkle.simulation import generate_strategy_grid\n",
    "\n",
    "CHUNKSIZE = 200\n",
    "FLUSH_EVERY = 1000\n",
    "PROCESSES = 16\n",
    "QUEUE_MAXSIZE = 50_000\n",
    "REPORT_EVERY = 100_000\n",
    "\n",
    "# Robust project-root detection (works in scripts, modules, notebooks)\n",
    "def find_project_root() -> Path:\n",
    "    try:\n",
    "        start = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        start = Path.cwd()\n",
    "    for p in (start, *start.parents):\n",
    "        if (p / \"pyproject.toml\").is_file():  # package folder marks root\n",
    "            return p\n",
    "    return Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_FILE = CHECKPOINT_DIR / \"win_counter.chk\"\n",
    "\n",
    "DEBUG_FIRST_N = 10000\n",
    "class FirstNFilter(logging.Filter):\n",
    "    def __init__(self, n: int = DEBUG_FIRST_N):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.seen: Counter[Tuple[str, int]] = Counter()\n",
    "    def filter(self, record: logging.LogRecord) -> bool:\n",
    "        key = (record.pathname, record.lineno)\n",
    "        self.seen[key] += 1\n",
    "        return self.seen[key] <= self.n\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(\n",
    "    \"%(asctime)s %(levelname)-5s | %(filename)s:%(lineno)d | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    "))\n",
    "handler.addFilter(FirstNFilter())\n",
    "root.handlers[:] = [handler]\n",
    "log = logging.getLogger(\"tournament\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "def producer(task_q: mp.Queue, n_games_per_player: int, num_strats: int) -> None:\n",
    "    log.debug(\"producer start n_games_per_player=%d num_strats=%d\", n_games_per_player, num_strats)\n",
    "    perm_rng = np.random.default_rng(999)\n",
    "    seed_rng = np.random.default_rng(1234)\n",
    "    log.debug(\"producer RNGs initialized\")\n",
    "    for _rep in range(n_games_per_player):\n",
    "        # log.debug(\"producer iteration rep=%d\", rep)\n",
    "        perm = perm_rng.permutation(num_strats)\n",
    "        for j in range(0, num_strats, 5):\n",
    "            # log.debug(\"producer inner j=%d\", j)\n",
    "            if j + 5 > num_strats:\n",
    "                log.debug(\"producer breaking inner loop at j=%d\", j)\n",
    "                break\n",
    "            seed = int(seed_rng.integers(2**32))\n",
    "            task = (seed, tuple(int(x) for x in perm[j:j+5]))\n",
    "            task_q.put(task)\n",
    "            # log.debug(\"producer enqueued task %s\", task)\n",
    "    log.debug(\"Finished 1st producer loop\")\n",
    "    for _ in range(PROCESSES):\n",
    "        task_q.put(None)\n",
    "        log.debug(\"producer sent sentinel None\")\n",
    "    log.debug(\"producer done\")\n",
    "\n",
    "def worker(strat_list: List, task_q: mp.Queue, result_q: mp.Queue) -> None:\n",
    "    log.debug(\"worker start\")\n",
    "    global strategies\n",
    "    strategies = strat_list\n",
    "    local_counter: Counter[str] = Counter()\n",
    "    processed = 0\n",
    "    while True:\n",
    "        batch: List[Tuple[int, Tuple[int, ...]]] = []\n",
    "        sentinel = False\n",
    "        for _ in range(CHUNKSIZE):\n",
    "            task = task_q.get()\n",
    "            log.debug(\"worker got task %s\", task)\n",
    "            if task is None:\n",
    "                log.debug(\"worker received sentinel None\")\n",
    "                sentinel = True\n",
    "                break\n",
    "            batch.append(task)\n",
    "        log.debug(\"worker batch size=%d\", len(batch))\n",
    "        if not batch and sentinel:\n",
    "            log.debug(\"worker exiting: empty batch and sentinel\")\n",
    "            break\n",
    "        for seed, idxs in batch:\n",
    "            row = _play_game(seed, [strategies[i] for i in idxs], 10_000)\n",
    "            winner_key = str(row[f\"{row['winner']}_strategy\"])\n",
    "            local_counter[winner_key] += 1\n",
    "            processed += 1\n",
    "            log.debug(\"worker processed game %d winner=%s\", processed, winner_key)\n",
    "            if processed % FLUSH_EVERY == 0:\n",
    "                result_q.put(local_counter)\n",
    "                log.debug(\"worker flushed local_counter at processed=%d\", processed)\n",
    "                local_counter = Counter()\n",
    "        if sentinel:\n",
    "            log.debug(\"worker exiting after sentinel\")\n",
    "            break\n",
    "    if local_counter:\n",
    "        result_q.put(local_counter)\n",
    "        log.debug(\"worker final flush local_counter\")\n",
    "    result_q.put(None)\n",
    "    log.debug(\"worker sent final sentinel None\")\n",
    "\n",
    "def collector(result_q: mp.Queue) -> None:\n",
    "    log.debug(\"collector start\")\n",
    "    win_counter: Counter[str] = Counter()\n",
    "    done_batches = 0\n",
    "    active_workers = PROCESSES\n",
    "    start = time.perf_counter()\n",
    "    while active_workers:\n",
    "        msg = result_q.get()\n",
    "        log.debug(\"collector got msg %s\", msg)\n",
    "        if msg is None:\n",
    "            active_workers -= 1\n",
    "            log.debug(\"collector worker done, active_workers=%d\", active_workers)\n",
    "            continue\n",
    "        win_counter.update(msg)\n",
    "        done_batches += 1\n",
    "        log.debug(\"collector updated win_counter with batch, done_batches=%d\", done_batches)\n",
    "        if done_batches % REPORT_EVERY == 0 or active_workers == 0:\n",
    "            hrs = (time.perf_counter() - start) / 3600\n",
    "            log.info(\"[batch %d] %.2f h elapsed\", done_batches, hrs)\n",
    "            with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "                pickle.dump({\"done\": done_batches, \"counter\": dict(win_counter)}, f)\n",
    "            log.debug(\"collector saved checkpoint at done_batches=%d\", done_batches)\n",
    "    df = pd.DataFrame({\n",
    "        \"strategy_idx\": range(len(strategies)),\n",
    "        \"str_repr\": [str(s) for s in strategies],\n",
    "    })\n",
    "    df[\"wincount\"] = df[\"str_repr\"].map(win_counter).fillna(0).astype(int)\n",
    "    df.to_csv(\"wincounts.csv\", index=False)\n",
    "    log.info(\"collector CSV written\")\n",
    "\n",
    "def main():\n",
    "    log.debug(\"main start\")\n",
    "    strategies, _ = generate_strategy_grid()\n",
    "    num_strats = len(strategies)\n",
    "    n_games_per_player = games_for_power(\n",
    "        n_strategies=num_strats,\n",
    "        delta=0.03,\n",
    "        alpha=0.025,\n",
    "        power=0.90,\n",
    "        method=\"bh\",\n",
    "        pairwise=True,\n",
    "    )\n",
    "    total_tasks = num_strats * n_games_per_player // 5\n",
    "    log.info(\"Grid: %d strategies, %d games/strat ⇒ %d tasks.\", num_strats, n_games_per_player, total_tasks)\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    task_q = ctx.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "    result_q = ctx.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "    threading.Thread(target=producer, args=(task_q, n_games_per_player, num_strats), daemon=True).start()\n",
    "    log.debug(\"producer thread started\")\n",
    "    threading.Thread(target=collector, args=(result_q,), daemon=True).start()\n",
    "    log.debug(\"collector thread started\")\n",
    "    processes = [ctx.Process(target=worker, args=(strategies, task_q, result_q)) for _ in range(PROCESSES)]\n",
    "    for p in processes:\n",
    "        p.start()\n",
    "        log.debug(\"worker process started pid=%s\", p.pid)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        log.debug(\"worker process joined pid=%s\", p.pid)\n",
    "    log.info(\"All workers joined – tournament complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c3086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:08:56 INFO  | MainProcess | Grid: 50 strategies, 2418 games/strat → 24180 tasks.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Parallel tournament runner for Farkle strategies.\n",
    "\n",
    "This script generates the default strategy grid and plays a powered number of\n",
    "round‐robin games using multiprocessing. Progress is periodically written to a\n",
    "checkpoint file so long runs can be resumed.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import threading\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Locate project root and ensure the package is importable\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    try:\n",
    "        start = Path(__file__).resolve()\n",
    "    except NameError:  # interactive session\n",
    "        start = Path.cwd()\n",
    "    for p in (start, *start.parents):\n",
    "        if (p / \"Src\" / \"Farkle\").is_dir():\n",
    "            return p\n",
    "    return Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from farkle.stats import games_for_power\n",
    "\n",
    "from farkle.simulation import generate_strategy_grid\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Constants\n",
    "# ---------------------------------------------------------------------------\n",
    "CHUNKSIZE = 200\n",
    "FLUSH_EVERY = 1000\n",
    "PROCESSES = 16\n",
    "QUEUE_MAXSIZE = 50_000\n",
    "REPORT_EVERY = 10\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_FILE = CHECKPOINT_DIR / \"win_counter.txt\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Logging\n",
    "# ---------------------------------------------------------------------------\n",
    "class FirstNFilter(logging.Filter):\n",
    "    \"\"\"Limit DEBUG spam to the first *n* occurrences per callsite.\"\"\"\n",
    "\n",
    "    def __init__(self, n: int = 10_000) -> None:\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.seen: Counter[Tuple[str, int]] = Counter()\n",
    "\n",
    "    def filter(self, record: logging.LogRecord) -> bool:  # noqa: D401 - bool\n",
    "        key = (record.pathname, record.lineno)\n",
    "        self.seen[key] += 1\n",
    "        return self.seen[key] <= self.n\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s %(levelname)-5s | %(processName)s | %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "    )\n",
    ")\n",
    "handler.addFilter(FirstNFilter())\n",
    "root.handlers[:] = [handler]\n",
    "log = logging.getLogger(\"tournament\")\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Workers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def producer(task_q: mp.Queue, n_games_per_player: int, num_strats: int) -> None:\n",
    "    \"\"\"Enqueue (seed, idx tuple) tasks for all tables.\"\"\"\n",
    "    perm_rng = np.random.default_rng(999)\n",
    "    seed_rng = np.random.default_rng(1234)\n",
    "    for _ in range(n_games_per_player):\n",
    "        perm = perm_rng.permutation(num_strats)\n",
    "        for j in range(0, num_strats, 5):\n",
    "            if j + 5 > num_strats:\n",
    "                break\n",
    "            seed = int(seed_rng.integers(2**32))\n",
    "            task_q.put((seed, tuple(int(x) for x in perm[j:j + 5])))\n",
    "    for _ in range(PROCESSES):\n",
    "        task_q.put(None)\n",
    "\n",
    "\n",
    "def worker(strat_list: List, task_q: mp.Queue, result_q: mp.Queue) -> None:\n",
    "    strategies = strat_list\n",
    "    local_counter: Counter[str] = Counter()\n",
    "    processed = 0\n",
    "    while True:\n",
    "        batch: List[Tuple[int, Tuple[int, ...]]] = []\n",
    "        sentinel = False\n",
    "        for _ in range(CHUNKSIZE):\n",
    "            task = task_q.get()\n",
    "            if task is None:\n",
    "                sentinel = True\n",
    "                break\n",
    "            batch.append(task)\n",
    "        if not batch and sentinel:\n",
    "            break\n",
    "        for seed, idxs in batch:\n",
    "            row = _play_game(seed, [strategies[i] for i in idxs], 10_000)\n",
    "            winner_key = str(row[f\"{row['winner']}_strategy\"])\n",
    "            local_counter[winner_key] += 1\n",
    "            processed += 1\n",
    "            if processed % FLUSH_EVERY == 0:\n",
    "                result_q.put(local_counter)\n",
    "                local_counter = Counter()\n",
    "        if sentinel:\n",
    "            break\n",
    "    if local_counter:\n",
    "        result_q.put(local_counter)\n",
    "    result_q.put(None)\n",
    "\n",
    "\n",
    "def collector(result_q: mp.Queue, num_strats: int, strategies: List) -> None:\n",
    "    win_counter: Counter[str] = Counter()\n",
    "    done_batches = 0\n",
    "    active_workers = PROCESSES\n",
    "    start = time.perf_counter()\n",
    "    while active_workers:\n",
    "        msg = result_q.get()\n",
    "        if msg is None:\n",
    "            active_workers -= 1\n",
    "            continue\n",
    "        win_counter.update(msg)\n",
    "        done_batches += 1\n",
    "        if done_batches % REPORT_EVERY == 0:\n",
    "            hrs = (time.perf_counter() - start) / 3600\n",
    "            log.info(\"[batch %d] %.2f h elapsed\", done_batches, hrs)\n",
    "            with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "                pickle.dump({\"done\": done_batches, \"counter\": dict(win_counter)}, f)\n",
    "    # final write\n",
    "    with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "        pickle.dump({\"done\": done_batches, \"counter\": dict(win_counter)}, f)\n",
    "    df = pd.DataFrame({\n",
    "        \"strategy_idx\": range(num_strats),\n",
    "        \"str_repr\": [str(s) for s in strategies],\n",
    "    })\n",
    "    df[\"wincount\"] = df[\"str_repr\"].map(win_counter).fillna(0).astype(int)\n",
    "    df.to_csv(\"wincounts.csv\", index=False)\n",
    "    log.info(\"collector CSV written\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Main entry\n",
    "# ---------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    strategies_0, _ = generate_strategy_grid()\n",
    "    strategies = strategies_0[0:50]\n",
    "    num_strats = len(strategies)\n",
    "    n_games_per_player = games_for_power(\n",
    "        n_strategies=num_strats,\n",
    "        delta=0.03,\n",
    "        alpha=0.3,\n",
    "        power=0.60,\n",
    "        method=\"bh\",\n",
    "        pairwise=True,\n",
    "    )\n",
    "    total_tasks = num_strats * n_games_per_player // 5\n",
    "    log.info(\n",
    "        \"Grid: %d strategies, %d games/strat → %d tasks.\",\n",
    "        num_strats,\n",
    "        n_games_per_player,\n",
    "        total_tasks,\n",
    "    )\n",
    "\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    task_q = ctx.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "    result_q = ctx.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "\n",
    "    prod = threading.Thread(\n",
    "        target=producer,\n",
    "        args=(task_q, n_games_per_player, num_strats),\n",
    "        daemon=False,\n",
    "    )\n",
    "    coll = threading.Thread(\n",
    "        target=collector,\n",
    "        args=(result_q, num_strats, strategies),\n",
    "        daemon=False,\n",
    "    )\n",
    "    prod.start()\n",
    "    coll.start()\n",
    "\n",
    "    processes = [\n",
    "        ctx.Process(target=worker, args=(strategies, task_q, result_q))\n",
    "        for _ in range(PROCESSES)\n",
    "    ]\n",
    "    for p in processes:\n",
    "        p.start()\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    prod.join()\n",
    "    coll.join()\n",
    "    log.info(\"All workers joined – tournament complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa72048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:53:41 INFO  | MainProcess | Grid: 8160 strategies, 10223 games/strat → 16683936 tasks.\n",
      "producer started\n",
      "collector started\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import threading\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Locate project root and ensure the package is importable\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    try:\n",
    "        start = Path(__file__).resolve()\n",
    "    except NameError:  # interactive session\n",
    "        start = Path.cwd()\n",
    "    for p in (start, *start.parents):\n",
    "        if (p / \"src\" / \"farkle\").is_dir():\n",
    "            return p\n",
    "    return Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from farkle.stats import games_for_power\n",
    "\n",
    "from farkle.simulation import generate_strategy_grid\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Constants\n",
    "# ---------------------------------------------------------------------------\n",
    "CHUNKSIZE = 200\n",
    "FLUSH_INITIAL = 1000\n",
    "FLUSH_INTERVAL = 8160\n",
    "PROCESSES = 16\n",
    "QUEUE_MAXSIZE = 50_000\n",
    "REPORT_EVERY = 1000\n",
    "CHECKPOINT_AFTER_DELIVERIES = 3\n",
    "CHECKPOINT_INTERVAL = 5 * 60  # seconds\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_FILE = CHECKPOINT_DIR / \"win_counter.chk\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Logging\n",
    "# ---------------------------------------------------------------------------\n",
    "class FirstNFilter(logging.Filter):\n",
    "    \"\"\"Limit DEBUG spam to the first *n* occurrences per callsite.\"\"\"\n",
    "\n",
    "    def __init__(self, n: int = 10_000) -> None:\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.seen: Counter[Tuple[str, int]] = Counter()\n",
    "\n",
    "    def filter(self, record: logging.LogRecord) -> bool:  # noqa: D401 - bool\n",
    "        key = (record.pathname, record.lineno)\n",
    "        self.seen[key] += 1\n",
    "        return self.seen[key] <= self.n\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s %(levelname)-5s | %(processName)s | %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "    )\n",
    ")\n",
    "handler.addFilter(FirstNFilter())\n",
    "root.handlers[:] = [handler]\n",
    "log = logging.getLogger(\"tournament\")\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Checkpoint helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_checkpoint() -> tuple[int, Counter[str]]:\n",
    "    \"\"\"Return ``(done_games, counter)`` if a checkpoint exists.\"\"\"\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        with CHECKPOINT_FILE.open(\"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data.get(\"done_games\", 0), Counter(data.get(\"counter\", {}))\n",
    "    return 0, Counter()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Workers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def producer(\n",
    "    task_q: mp.Queue, n_games_per_player: int, num_strats: int, start: int = 0\n",
    ") -> None:\n",
    "    \"\"\"Enqueue (seed, idx tuple) tasks for all tables.\"\"\"\n",
    "    perm_rng = np.random.default_rng(999)\n",
    "    seed_rng = np.random.default_rng(1234)\n",
    "    tasks_per_round = num_strats // 5\n",
    "\n",
    "    skip_rounds, skip_within = divmod(start, tasks_per_round)\n",
    "    for _ in range(skip_rounds):\n",
    "        perm_rng.permutation(num_strats)\n",
    "        for _ in range(tasks_per_round):\n",
    "            seed_rng.integers(2**32)\n",
    "\n",
    "    for round_idx in range(skip_rounds, n_games_per_player):\n",
    "        perm = perm_rng.permutation(num_strats)\n",
    "        start_step = skip_within if round_idx == skip_rounds else 0\n",
    "        for step in range(start_step, tasks_per_round):\n",
    "            seed = int(seed_rng.integers(2**32))\n",
    "            task_q.put((seed, tuple(int(x) for x in perm[step * 5 : step * 5 + 5])))\n",
    "        skip_within = 0\n",
    "    for _ in range(PROCESSES):\n",
    "        task_q.put(None)\n",
    "\n",
    "\n",
    "def worker(strat_list: List, task_q: mp.Queue, result_q: mp.Queue) -> None:\n",
    "    strategies = strat_list\n",
    "    local_counter: Counter[str] = Counter()\n",
    "    processed = 0\n",
    "    next_flush = FLUSH_INITIAL\n",
    "    while True:\n",
    "        batch: List[Tuple[int, Tuple[int, ...]]] = []\n",
    "        sentinel = False\n",
    "        for _ in range(CHUNKSIZE):\n",
    "            task = task_q.get()\n",
    "            if task is None:\n",
    "                sentinel = True\n",
    "                break\n",
    "            batch.append(task)\n",
    "        if not batch and sentinel:\n",
    "            break\n",
    "        for seed, idxs in batch:\n",
    "            row = _play_game(seed, [strategies[i] for i in idxs], 10_000)\n",
    "            winner_key = str(row[f\"{row['winner']}_strategy\"])\n",
    "            local_counter[winner_key] += 1\n",
    "            processed += 1\n",
    "            if processed >= next_flush:\n",
    "                result_q.put(local_counter)\n",
    "                local_counter = Counter()\n",
    "                next_flush += FLUSH_INTERVAL\n",
    "        if sentinel:\n",
    "            break\n",
    "    if local_counter:\n",
    "        result_q.put(local_counter)\n",
    "    result_q.put(None)\n",
    "\n",
    "\n",
    "def collector(\n",
    "    result_q: mp.Queue,\n",
    "    num_strats: int,\n",
    "    strategies: List,\n",
    "    done_games: int = 0,\n",
    "    win_counter: Counter[str] | None = None,\n",
    ") -> None:\n",
    "    win_counter = win_counter or Counter()\n",
    "    deliveries = 0\n",
    "    active_workers = PROCESSES\n",
    "    start = time.perf_counter()\n",
    "    last_checkpoint = time.perf_counter()\n",
    "    first_checkpoint = False\n",
    "    while active_workers:\n",
    "        msg = result_q.get()\n",
    "        if msg is None:\n",
    "            active_workers -= 1\n",
    "            continue\n",
    "        win_counter.update(msg)\n",
    "        deliveries += 1\n",
    "        done_games += sum(msg.values())\n",
    "        if deliveries % REPORT_EVERY == 0:\n",
    "            hrs = (time.perf_counter() - start) / 3600\n",
    "            log.info(\"[delivery %d] %.2f h elapsed\", deliveries, hrs)\n",
    "        now = time.perf_counter()\n",
    "        write_checkpoint = False\n",
    "        if not first_checkpoint and deliveries >= CHECKPOINT_AFTER_DELIVERIES:\n",
    "            write_checkpoint = True\n",
    "            first_checkpoint = True\n",
    "        elif first_checkpoint and now - last_checkpoint >= CHECKPOINT_INTERVAL:\n",
    "            write_checkpoint = True\n",
    "        if write_checkpoint:\n",
    "            with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "                pickle.dump({\"done_games\": done_games, \"counter\": dict(win_counter)}, f)\n",
    "            last_checkpoint = now\n",
    "    # final write\n",
    "    with CHECKPOINT_FILE.open(\"wb\") as f:\n",
    "        pickle.dump({\"done\": done_batches, \"counter\": dict(win_counter)}, f)\n",
    "        pickle.dump({\"done_games\": done_games, \"counter\": dict(win_counter)}, f)\n",
    "    df = pd.DataFrame({\n",
    "        \"strategy_idx\": range(num_strats),\n",
    "        \"str_repr\": [str(s) for s in strategies],\n",
    "    })\n",
    "    df[\"wincount\"] = df[\"str_repr\"].map(win_counter).fillna(0).astype(int)\n",
    "    df.to_csv(\"wincounts.csv\", index=False)\n",
    "    log.info(\"collector CSV written\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Main entry\n",
    "# ---------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    strategies, _ = generate_strategy_grid()\n",
    "    num_strats = len(strategies)\n",
    "    n_games_per_player = games_for_power(\n",
    "        n_strategies=num_strats,\n",
    "        delta=0.03,\n",
    "        alpha=0.025,\n",
    "        power=0.90,\n",
    "        method=\"bh\",\n",
    "        pairwise=True,\n",
    "    )\n",
    "    tasks_per_round = num_strats // 5\n",
    "    total_tasks = tasks_per_round * n_games_per_player\n",
    "    log.info(\n",
    "        \"Grid: %d strategies, %d games/strat → %d tasks.\",\n",
    "        num_strats,\n",
    "        n_games_per_player,\n",
    "        total_tasks,\n",
    "    )\n",
    "\n",
    "    done_games, counter = load_checkpoint()\n",
    "    if done_games:\n",
    "        log.info(\"Resuming from checkpoint with %d completed games\", done_games)\n",
    "\n",
    "    if done_games >= total_tasks:\n",
    "        log.info(\"Checkpoint already complete – writing CSV and exiting.\")\n",
    "        df = pd.DataFrame({\n",
    "            \"strategy_idx\": range(num_strats),\n",
    "            \"str_repr\": [str(s) for s in strategies],\n",
    "        })\n",
    "        df[\"wincount\"] = df[\"str_repr\"].map(counter).fillna(0).astype(int)\n",
    "        df.to_csv(\"wincounts.csv\", index=False)\n",
    "        sys.exit(0)\n",
    "\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    task_q = ctx.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "    result_q = ctx.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "\n",
    "    prod = threading.Thread(\n",
    "        target=producer,\n",
    "        args=(task_q, n_games_per_player, num_strats, done_games),\n",
    "        daemon=False,\n",
    "    )\n",
    "    coll = threading.Thread(\n",
    "        target=collector,\n",
    "        args=(result_q, num_strats, strategies, done_games, counter),\n",
    "        daemon=False,\n",
    "    )\n",
    "    prod.start()\n",
    "    print(\"producer started\")\n",
    "    coll.start()\n",
    "    print(\"collector started\")\n",
    "\n",
    "    processes = [\n",
    "        ctx.Process(target=worker, args=(strategies, task_q, result_q))\n",
    "        for _ in range(PROCESSES)\n",
    "    ]\n",
    "    for p in processes:\n",
    "        p.start()\n",
    "        print(\"worker started\")\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    prod.join()\n",
    "    coll.join()\n",
    "    log.info(\"All workers joined – tournament complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60c27e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
